{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f27b68-b9a3-4fbe-85f5-2f5fea18eb3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --q vectice[github]==22.3.5.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "393075ac-6488-401e-a508-586c2d754b1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: vectice\n",
      "Version: 22.3.5.1\n",
      "Summary: Vectice Python library\n",
      "Home-page: https://www.vectice.com\n",
      "Author: Vectice Inc.\n",
      "Author-email: sdk@vectice.com\n",
      "License: Apache License 2.0\n",
      "Location: /opt/conda/lib/python3.7/site-packages\n",
      "Requires: python-dotenv, requests, urllib3\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip show vectice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9e165f1",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.098471,
     "end_time": "2022-01-15T09:50:45.169883",
     "exception": false,
     "start_time": "2022-01-15T09:50:45.071412",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3582c1",
   "metadata": {},
   "source": [
    "The training set has 125,497,040 (125 Million) rows and 6 columns: row id, date, store number, item number, unit sales (keep in mind that this can be an integer, float, or -1, which represents a returned item), and whether there was a promotion for a particular item.\n",
    "\n",
    "Now here we will take some amount of the data for analysis purpose because we have large number of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e958f3d0",
   "metadata": {
    "papermill": {
     "duration": 45.018469,
     "end_time": "2022-01-15T09:53:19.597017",
     "exception": false,
     "start_time": "2022-01-15T09:52:34.578548",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "items = pd.read_csv(\"items.csv\")\n",
    "holiday_events = pd.read_csv(\"holidays_events.csv\", parse_dates=['date'])\n",
    "stores = pd.read_csv(\"stores.csv\")\n",
    "oil = pd.read_csv(\"oil.csv\", parse_dates=['date'])\n",
    "transactions = pd.read_csv(\"transactions.csv\", parse_dates=['date'])\n",
    "# the full training data's output: \"125,497,040 rows | 6 columns\"\n",
    "#Here, we only load approx 5% of the data just to get a rough idea of what is in store for us.\n",
    "train = pd.read_csv(\"train_trimmed.csv\", parse_dates=['date'])\n",
    "train_large = pd.read_csv('train_large.csv', parse_dates = ['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c9fa350",
   "metadata": {
    "papermill": {
     "duration": 0.111984,
     "end_time": "2022-01-15T09:53:19.797845",
     "exception": false,
     "start_time": "2022-01-15T09:53:19.685861",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>store_nbr</th>\n",
       "      <th>item_nbr</th>\n",
       "      <th>unit_sales</th>\n",
       "      <th>onpromotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>25</td>\n",
       "      <td>103665</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>25</td>\n",
       "      <td>105574</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>25</td>\n",
       "      <td>105575</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>25</td>\n",
       "      <td>108079</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>25</td>\n",
       "      <td>108701</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  id       date  store_nbr  item_nbr  unit_sales  onpromotion\n",
       "0           0   0 2013-01-01         25    103665         7.0          NaN\n",
       "1           1   1 2013-01-01         25    105574         1.0          NaN\n",
       "2           2   2 2013-01-01         25    105575         2.0          NaN\n",
       "3           3   3 2013-01-01         25    108079         1.0          NaN\n",
       "4           4   4 2013-01-01         25    108701         1.0          NaN"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b45c0005",
   "metadata": {
    "papermill": {
     "duration": 0.109819,
     "end_time": "2022-01-15T09:53:19.998294",
     "exception": false,
     "start_time": "2022-01-15T09:53:19.888475",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nulls in Oil columns: ['date' 'dcoilwtico'] => [False  True]\n",
      "======================================================================\n",
      "Nulls in holiday_events columns: ['date' 'type' 'locale' 'locale_name' 'description' 'transferred'] => [False False False False False False]\n",
      "======================================================================\n",
      "Nulls in stores columns: ['store_nbr' 'city' 'state' 'type' 'cluster'] => [False False False False False]\n",
      "======================================================================\n",
      "Nulls in transactions columns: ['date' 'store_nbr' 'transactions'] => [False False False]\n"
     ]
    }
   ],
   "source": [
    "print(\"Nulls in Oil columns: {0} => {1}\".format(oil.columns.values,oil.isnull().any().values))\n",
    "print(\"=\"*70)\n",
    "print(\"Nulls in holiday_events columns: {0} => {1}\".format(holiday_events.columns.values,holiday_events.isnull().any().values))\n",
    "print(\"=\"*70)\n",
    "print(\"Nulls in stores columns: {0} => {1}\".format(stores.columns.values,stores.isnull().any().values))\n",
    "print(\"=\"*70)\n",
    "print(\"Nulls in transactions columns: {0} => {1}\".format(transactions.columns.values,transactions.isnull().any().values))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc27958",
   "metadata": {
    "papermill": {
     "duration": 0.089847,
     "end_time": "2022-01-15T09:53:20.180143",
     "exception": false,
     "start_time": "2022-01-15T09:53:20.090296",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    " The only missing data occurs in the oil data file, which provides the historical daily price for oil."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f8f2d0",
   "metadata": {
    "papermill": {
     "duration": 0.167189,
     "end_time": "2022-01-15T09:54:03.229848",
     "exception": false,
     "start_time": "2022-01-15T09:54:03.062659",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57be4cc9",
   "metadata": {
    "papermill": {
     "duration": 0.167783,
     "end_time": "2022-01-15T09:54:03.562802",
     "exception": false,
     "start_time": "2022-01-15T09:54:03.395019",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Here we analyze the data and select the features for our model to be trained on.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34952541",
   "metadata": {
    "papermill": {
     "duration": 0.165965,
     "end_time": "2022-01-15T09:54:03.894763",
     "exception": false,
     "start_time": "2022-01-15T09:54:03.728798",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Train**\n",
    "id, date, store_nbr, item_nbr, unit_scale, on_promotion\n",
    "\n",
    "**Items**\n",
    "item_nbr, family, class, perishable\n",
    "\n",
    "**Holidays_events**\n",
    "date, type, locale, locale_name, description, transferred\n",
    "\n",
    "**Stores**\n",
    "store_nbr, city, state, type, cluster\n",
    "\n",
    "**Oil**\n",
    "date, dcoilwtico\n",
    "\n",
    "**Transactions**\n",
    "date, store_nbr, transactions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba9d038",
   "metadata": {
    "papermill": {
     "duration": 0.168723,
     "end_time": "2022-01-15T09:54:04.231620",
     "exception": false,
     "start_time": "2022-01-15T09:54:04.062897",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Selected features as inputs to the model**\n",
    "\n",
    "date, holiday.type, holidaye.locale, holiday.locale_name, holiday_transfered, store_nbr, store.city, store.state, store.type, store.cluster, transactions, item_nbr, item.family, item.class, on_promotion, perishable, dcoilwtico.\n",
    "\n",
    "**Selected features as outputs of the model**\n",
    "\n",
    "transactions per store, unit_sales per item"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5b8ca6",
   "metadata": {
    "papermill": {
     "duration": 0.167221,
     "end_time": "2022-01-15T09:54:04.567001",
     "exception": false,
     "start_time": "2022-01-15T09:54:04.399780",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# DATA pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929abc39",
   "metadata": {},
   "source": [
    "Here, we'll merge the different dataframes into one dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5c19912",
   "metadata": {
    "papermill": {
     "duration": 0.279687,
     "end_time": "2022-01-15T09:54:05.347688",
     "exception": false,
     "start_time": "2022-01-15T09:54:05.068001",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class prepare_data(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        print(\"prepare_data -> init\")\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        train_stores = X[0].merge(X[1], right_on = 'store_nbr', left_on='store_nbr')\n",
    "        train_stores_oil = train_stores.merge(X[2], right_on='date', left_on='date')\n",
    "        train_stores_oil_items = train_stores_oil.merge(X[3], right_on = 'item_nbr', left_on = 'item_nbr')\n",
    "        train_stores_oil_items_transactions = train_stores_oil_items.merge(X[4], right_on = ['date', 'store_nbr'], left_on = ['date', 'store_nbr'])\n",
    "        train_stores_oil_items_transactions_hol = train_stores_oil_items_transactions.merge(X[5], right_on = 'date', left_on = 'date')\n",
    "        \n",
    "        data_df = train_stores_oil_items_transactions_hol.copy(deep = True)\n",
    "        \n",
    "        # change the bool to int\n",
    "        data_df['onpromotion'] = data_df['onpromotion'].astype(int)\n",
    "        data_df['transferred'] = data_df['transferred'].astype(int)\n",
    "\n",
    "        # change the names\n",
    "        data_df.rename(columns={'type_x': 'st_type', 'type_y': 'hol_type'}, inplace=True)\n",
    "\n",
    "        # drop the id\n",
    "        data_df.drop(['id'], axis=1, inplace=True)\n",
    "        \n",
    "        print(data_df.head())\n",
    "        \n",
    "        # handle date\n",
    "        data_df['date'] = pd.to_datetime(data_df['date'])\n",
    "        data_df['date'] = data_df['date'].map(dt.datetime.toordinal)\n",
    "                \n",
    "        return data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43c3971",
   "metadata": {
    "papermill": {
     "duration": 0.167427,
     "end_time": "2022-01-15T09:54:05.681875",
     "exception": false,
     "start_time": "2022-01-15T09:54:05.514448",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Custom transform for splitting the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d3728a-e00f-4187-8bed-10fe06578696",
   "metadata": {},
   "source": [
    "Here, we split dataframe into numerical values, categorical values and date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0cb5d49",
   "metadata": {
    "papermill": {
     "duration": 0.177586,
     "end_time": "2022-01-15T09:54:06.025656",
     "exception": false,
     "start_time": "2022-01-15T09:54:05.848070",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# split dataframe into numerical values, categorical values and date\n",
    "class split_data(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        print(\"split_data -> init\")\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        # Get columns for each type         \n",
    "        df_ = X.drop(['date'], axis = 1)\n",
    "        cols = df_.columns\n",
    "        num_cols = df_._get_numeric_data().columns\n",
    "        cat_cols = list(set(cols) - set(num_cols))\n",
    "        \n",
    "        data_num_df = X[num_cols]\n",
    "        data_cat_df = X[cat_cols]\n",
    "        data_date_df = X['date']\n",
    "        \n",
    "        return data_num_df, data_cat_df, data_date_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f7a254",
   "metadata": {
    "papermill": {
     "duration": 0.165333,
     "end_time": "2022-01-15T09:54:06.357635",
     "exception": false,
     "start_time": "2022-01-15T09:54:06.192302",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Herre, we handle the missing data, apply standard scaler to numerical attributes, and convert categorical data into numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "afa81987",
   "metadata": {
    "papermill": {
     "duration": 0.41475,
     "end_time": "2022-01-15T09:54:06.937863",
     "exception": false,
     "start_time": "2022-01-15T09:54:06.523113",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class process_data(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        print(\"process_data -> init\")\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        ### numerical data\n",
    "        # impute nulls in numerical attributes\n",
    "        imputer = SimpleImputer(strategy=\"mean\", copy=\"true\")\n",
    "        num_imp = imputer.fit_transform(X[0])\n",
    "        data_num_df = pd.DataFrame(num_imp, columns=X[0].columns, index=X[0].index)\n",
    "        \n",
    "        # apply standard scaling\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(data_num_df)\n",
    "        num_scaled = scaler.transform(data_num_df)\n",
    "        data_num_df = pd.DataFrame(num_scaled, columns=X[0].columns, index=X[0].index)\n",
    "        \n",
    "        ### categorical data\n",
    "        # one hot encoder\n",
    "        cat_encoder = OneHotEncoder(sparse=False)\n",
    "        data_cat_1hot = cat_encoder.fit_transform(X[1])\n",
    "        \n",
    "        # convert it to datafram with n*99 where n number of rows and 99 is no. of categories\n",
    "        data_cat_df = pd.DataFrame(data_cat_1hot, columns=cat_encoder.get_feature_names()) #, index=X[1].index)\n",
    "                \n",
    "        return data_num_df, data_cat_df, X[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f3356b34",
   "metadata": {
    "papermill": {
     "duration": 0.176369,
     "end_time": "2022-01-15T09:54:07.611183",
     "exception": false,
     "start_time": "2022-01-15T09:54:07.434814",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class join_df(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        print(\"join_df -> init\")\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        ### numerical data\n",
    "        data_df = X[0].join(X[1])\n",
    "        data_df = data_df.join(X[2])\n",
    "        \n",
    "        return data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1550ae6-6d93-49ce-9dff-9835f3a1f027",
   "metadata": {},
   "source": [
    "Applying the Pipeline to get our prepared data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16f372ad",
   "metadata": {
    "papermill": {
     "duration": 31.91009,
     "end_time": "2022-01-15T09:54:39.688083",
     "exception": false,
     "start_time": "2022-01-15T09:54:07.777993",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prepare_data -> init\n",
      "split_data -> init\n",
      "process_data -> init\n",
      "join_df -> init\n",
      "   Unnamed: 0       date  store_nbr  item_nbr  unit_sales  onpromotion  \\\n",
      "0      405070 2017-05-12         31    877514         2.0            0   \n",
      "1      405071 2017-05-12         31    881701         6.0            0   \n",
      "2      405074 2017-05-12         31    882624        12.0            0   \n",
      "3      405076 2017-05-12         31    886396         1.0            0   \n",
      "4      405077 2017-05-12         31    888763         4.0            1   \n",
      "\n",
      "       city     state st_type  cluster  dcoilwtico     family  class  \\\n",
      "0  Babahoyo  Los Rios       B       10       47.83   CLEANING   3020   \n",
      "1  Babahoyo  Los Rios       B       10       47.83  GROCERY I   1042   \n",
      "2  Babahoyo  Los Rios       B       10       47.83      DAIRY   2116   \n",
      "3  Babahoyo  Los Rios       B       10       47.83  GROCERY I   1042   \n",
      "4  Babahoyo  Los Rios       B       10       47.83       DELI   2646   \n",
      "\n",
      "   perishable  transactions hol_type locale locale_name  \\\n",
      "0           0          1363  Holiday  Local        Puyo   \n",
      "1           0          1363  Holiday  Local        Puyo   \n",
      "2           1          1363  Holiday  Local        Puyo   \n",
      "3           0          1363  Holiday  Local        Puyo   \n",
      "4           1          1363  Holiday  Local        Puyo   \n",
      "\n",
      "              description  transferred  \n",
      "0  Cantonizacion del Puyo            0  \n",
      "1  Cantonizacion del Puyo            0  \n",
      "2  Cantonizacion del Puyo            0  \n",
      "3  Cantonizacion del Puyo            0  \n",
      "4  Cantonizacion del Puyo            0  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "pipe_processing = Pipeline([\n",
    "        ('prepare_data', prepare_data()),\n",
    "        ('split_data', split_data()),\n",
    "        ('process_data', process_data()),\n",
    "        ('join_data', join_df())\n",
    "    ])\n",
    "\n",
    "# our prepared data\n",
    "data_df = pipe_processing.fit_transform([train_large, stores, oil, items, transactions, holiday_events])\n",
    "\n",
    "# split it according to our feature engineering\n",
    "X = data_df.drop(['unit_sales', 'transactions'], axis=1)\n",
    "Y = data_df[['unit_sales', 'transactions']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aae45a7",
   "metadata": {
    "papermill": {
     "duration": 0.169001,
     "end_time": "2022-01-15T09:54:40.023332",
     "exception": false,
     "start_time": "2022-01-15T09:54:39.854331",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Generate test and training data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "60d81102",
   "metadata": {
    "papermill": {
     "duration": 1.579671,
     "end_time": "2022-01-15T09:54:41.769786",
     "exception": false,
     "start_time": "2022-01-15T09:54:40.190115",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f8290c-1a53-41e6-bb31-1209a0c4aac1",
   "metadata": {},
   "source": [
    "## Auhtenticate to Vectice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "26ac3201",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the required packages\n",
    "from vectice import Experiment\n",
    "from vectice.api.json import ModelType\n",
    "from vectice.api.json import JobType\n",
    "from vectice.api.json import JobArtifactType\n",
    "from vectice.api.json import ModelVersionStatus\n",
    "from vectice.api.json import VersionStrategy\n",
    "import logging\n",
    "import os\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "os.environ['VECTICE_API_ENDPOINT']= \"app.vectice.com\"\n",
    "\n",
    "os.environ['VECTICE_API_TOKEN'] = \"Token\"\n",
    "\n",
    "# Add you project id. The project id can be found in the project settings page in the Vectice UI\n",
    "project_id = ID"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ee6fbe-e8d8-44db-b3d0-5424edbdc2ab",
   "metadata": {
    "tags": []
   },
   "source": [
    "Creating our modeling experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354fa5d8-09c6-475d-95a6-f1df3bfa2a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = Experiment(job=\"Modeling\", project=project_id, job_type=JobType.TRAINING, auto_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9f50e985",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Inputs\n",
    "train_ds = experiment.add_dataset_version(dataset=\"cleaned_kc_house_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd0828a",
   "metadata": {
    "papermill": {
     "duration": 0.167934,
     "end_time": "2022-01-15T09:54:42.104831",
     "exception": false,
     "start_time": "2022-01-15T09:54:41.936897",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Modelling and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18129c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error,mean_absolute_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df65f31",
   "metadata": {
    "papermill": {
     "duration": 0.168275,
     "end_time": "2022-01-15T09:54:43.166038",
     "exception": false,
     "start_time": "2022-01-15T09:54:42.997763",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "44b685c4-3bc0-4cde-a8cf-2a27a444065b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root_mean_squared_error:  0.7599447094988461\n",
      "mean_absolute_error:  0.31084975149783933\n"
     ]
    }
   ],
   "source": [
    "experiment.start()\n",
    "\n",
    "model = LinearRegression()\n",
    "\n",
    "model.fit(x_train.values, y_train.values)\n",
    "    \n",
    "pred = model.predict(x_test.values)\n",
    "    \n",
    "RMSE = np.sqrt(mean_squared_error(y_test.values, pred))\n",
    "MAE = mean_absolute_error(y_test.values, pred)\n",
    "\n",
    "print(\"root_mean_squared_error: \",RMSE) \n",
    "print(\"mean_absolute_error: \", MAE)\n",
    "\n",
    "# Let's log the model we trained along with its metrics, as a new version \n",
    "# of the \"Regressor\" model in Vectice.\n",
    "\n",
    "metrics = {\"RMSE\": RMSE, \"MAE\": MAE}\n",
    "model_version = experiment.add_model_version(model=\"Regressor\", algorithm=\"Linear Regression\", metrics=metrics)\n",
    "\n",
    "# We complete the current experiment's run \n",
    "## The created model version will be automatically attached as output of the run\n",
    "experiment.complete()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04ae234-b668-45d8-8cd2-e99262370330",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "37740f99-26a6-4db1-a48b-1340acb2e7fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n",
      "mean_squared_error:  0.6279706411305517\n",
      "mean_absolute_error:  0.36668117625129293\n"
     ]
    }
   ],
   "source": [
    "experiment.start(inputs=[train_ds])\n",
    "\n",
    "print(\"Random Forest\")\n",
    "model = RandomForestRegressor(random_state=42)\n",
    "\n",
    "model.fit(x_train.values, y_train.values)\n",
    "    \n",
    "pred = model.predict(x_test.values)\n",
    "    \n",
    "RMSE = np.sqrt(mean_squared_error(y_test.values, pred))\n",
    "MAE = mean_absolute_error(y_test.values, pred)\n",
    "\n",
    "print(\"root_mean_squared_error: \",RMSE) \n",
    "print(\"mean_absolute_error: \", MAE)\n",
    "\n",
    "# Let's log the model we trained along with its metrics, as a new version \n",
    "# of the \"Regressor\" model in Vectice.\n",
    "\n",
    "metrics = {\"RMSE\": RMSE, \"MAE\": MAE}\n",
    "model_version = experiment.add_model_version(model=\"Regressor\", algorithm=\"Linear Regression\", metrics=metrics)\n",
    "\n",
    "# We complete the current experiment's run \n",
    "## The created model version will be automatically attached as output of the run\n",
    "experiment.complete()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c94f90-81cf-4c49-855b-2370942d6a39",
   "metadata": {},
   "source": [
    "Random forest model has the lowest error"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m94",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m94"
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 ('aml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 4527.062431,
   "end_time": "2022-01-15T11:06:02.218444",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-01-15T09:50:35.156013",
   "version": "2.3.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "f3244003ce06b8d4815ef5829ab9017a925cc64cacb0a80d02b1f6e20420f2b3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
